{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOu_X5XG6aGJ",
        "outputId": "94020e6e-6924-4425-c71b-38a738733739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ucimlrepo in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (0.0.7)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (from ucimlrepo) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (from ucimlrepo) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\envs\\babyemotion\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5opjko0xJwKi"
      },
      "source": [
        "# Regression Datsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i4ZalT6w7ENE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "egkS00vH7JNX"
      },
      "outputs": [],
      "source": [
        "# Define activation and utility functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def get_all_H(X, W_list, b_list):\n",
        "    H_list = []\n",
        "    H = sigmoid(X @ W_list[0] + b_list[0])\n",
        "    H_list.append(H)\n",
        "    for l in range(1, len(W_list)):\n",
        "        XH = np.concatenate([X, H_list[-1]], axis=1)\n",
        "        H = sigmoid(XH @ W_list[l] + b_list[l])\n",
        "        H_list.append(H)\n",
        "    return H_list\n",
        "\n",
        "def get_D_deep(X, H_list, l):\n",
        "    return np.concatenate([X, H_list[l]], axis=1)\n",
        "\n",
        "def init_fit(X0, y0, in_dim, hid_dim, L, lam, rng):\n",
        "    beta_list, P_list, W_list, b_list = [], [], [], []\n",
        "    input_dims = [in_dim] + [in_dim + hid_dim] * (L - 1)\n",
        "    for l in range(L):\n",
        "        W = rng.standard_normal((input_dims[l], hid_dim))\n",
        "        b = rng.standard_normal((hid_dim,))\n",
        "        W_list.append(W)\n",
        "        b_list.append(b)\n",
        "    H_list = get_all_H(X0, W_list, b_list)\n",
        "    for l in range(L):\n",
        "        D = get_D_deep(X0, H_list, l)\n",
        "        P = np.linalg.inv(D.T @ D + lam * np.eye(D.shape[1]))\n",
        "        beta = P @ D.T @ y0\n",
        "        P_list.append(P)\n",
        "        beta_list.append(beta)\n",
        "    return beta_list, P_list, W_list, b_list\n",
        "\n",
        "def partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L):\n",
        "    H_list = get_all_H(Xk, W_list, b_list)\n",
        "    for l in range(L):\n",
        "        Dk = get_D_deep(Xk, H_list, l)\n",
        "        P = P_list[l]\n",
        "        beta = beta_list[l]\n",
        "        P_new = P - P @ Dk.T @ np.linalg.inv(np.eye(Dk.shape[0]) + Dk @ P @ Dk.T) @ Dk @ P\n",
        "        beta_new = beta + P_new @ Dk.T @ (yk - Dk @ beta)\n",
        "        P_list[l] = P_new\n",
        "        beta_list[l] = beta_new\n",
        "\n",
        "def predict_average(X, beta_list, W_list, b_list, L):\n",
        "    H_list = get_all_H(X, W_list, b_list)\n",
        "    preds = []\n",
        "    for l in range(L):\n",
        "        D = get_D_deep(X, H_list, l)\n",
        "        preds.append(D @ beta_list[l])\n",
        "    return np.mean(preds, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-hwf1gjkAdh"
      },
      "source": [
        "## Airfoil_Noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFiopLdB7ORx",
        "outputId": "3b021483-0f94-42cf-e277-37dd910c6799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature sample:\n",
            "        0    1       2     3         4\n",
            "0   800.0  0.0  0.3048  71.3  0.002663\n",
            "1  1000.0  0.0  0.3048  71.3  0.002663\n",
            "2  1250.0  0.0  0.3048  71.3  0.002663\n",
            "3  1600.0  0.0  0.3048  71.3  0.002663\n",
            "4  2000.0  0.0  0.3048  71.3  0.002663\n",
            "5  2500.0  0.0  0.3048  71.3  0.002663\n",
            "6  3150.0  0.0  0.3048  71.3  0.002663\n",
            "7  4000.0  0.0  0.3048  71.3  0.002663\n",
            "8  5000.0  0.0  0.3048  71.3  0.002663\n",
            "9  6300.0  0.0  0.3048  71.3  0.002663\n",
            "\n",
            "Dataset dimensions: 1503 rows Ã— 5 columns\n",
            "\n",
            "Train set: 1052 samples\n",
            "Test set : 451 samples\n"
          ]
        }
      ],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Fetch dataset from UCI repo by ID (291 = Airfoil Self Noise)\n",
        "airfoil_self_noise = fetch_ucirepo(id=291)\n",
        "\n",
        "# Step 2: Extract features and target\n",
        "X = airfoil_self_noise.data.features.to_numpy()\n",
        "y = airfoil_self_noise.data.targets.to_numpy().astype(np.float32).ravel()\n",
        "\n",
        "# Step 3: Metadata & variable info (optional)\n",
        "print(\"\\nFeature sample:\")\n",
        "print(pd.DataFrame(X).head(10))\n",
        "print(f\"\\nDataset dimensions: {X.shape[0]} rows Ã— {X.shape[1]} columns\")\n",
        "\n",
        "# Step 4: 70:30 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42\n",
        ")\n",
        "\n",
        "# Confirm dimensions\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set : {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Best RMSE = 2.5073 Â± 0.2197\n",
            "    at L=2, N=1024, Î»=2.50e-01, batch_size=22\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Standardization (fit only on training data)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Hyperparameter search space\n",
        "lam_powers = list(range(-6, 13, 2))                         # exponents for 2^x\n",
        "lam_values = [1 / (2 ** x) for x in lam_powers]             # regularization values\n",
        "L_values = list(range(2, 10))                               # number of hidden layers\n",
        "N_values = [256, 512, 1024]                                 # number of hidden units\n",
        "batch_sizes = [20, 22, 24, 26, 28, 30]                      # online batch sizes\n",
        "\n",
        "# Step 3: Setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "best_rmse = float(\"inf\")\n",
        "best_config = None\n",
        "results = []\n",
        "\n",
        "# Step 4: Grid search\n",
        "for L in L_values:\n",
        "    for N in N_values:\n",
        "        for lam in lam_values:\n",
        "            for batch_size in batch_sizes:\n",
        "                fold_rmse = []\n",
        "                for train_idx, val_idx in kf.split(X_train):\n",
        "                    X_fold_train, X_val = X_train[train_idx], X_train[val_idx]\n",
        "                    y_fold_train, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "                    # Initial batch\n",
        "                    init_size = len(X_fold_train) // 2\n",
        "                    X0, y0 = X_fold_train[:init_size], y_fold_train[:init_size]\n",
        "\n",
        "                    beta_list, P_list, W_list, b_list = init_fit(\n",
        "                        X0, y0, X.shape[1], N, L, lam, rng\n",
        "                    )\n",
        "\n",
        "                    # Online Update\n",
        "                    for i in range(init_size, len(X_fold_train), batch_size):\n",
        "                        Xk = X_fold_train[i:i+batch_size]\n",
        "                        yk = y_fold_train[i:i+batch_size]\n",
        "                        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L)\n",
        "\n",
        "                    # Validation prediction\n",
        "                    pred_val = predict_average(X_val, beta_list, W_list, b_list, L)\n",
        "                    rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n",
        "                    fold_rmse.append(rmse)\n",
        "\n",
        "                mean_rmse = np.mean(fold_rmse)\n",
        "                std_rmse = np.std(fold_rmse)\n",
        "                results.append((L, N, lam, batch_size, mean_rmse, std_rmse))\n",
        "\n",
        "                if mean_rmse < best_rmse:\n",
        "                    best_rmse = mean_rmse\n",
        "                    best_config = (L, N, lam, batch_size, std_rmse)\n",
        "\n",
        "# Step 6: Report\n",
        "print(f\"\\nâœ… Best RMSE = {best_rmse:.4f} Â± {best_config[4]:.4f}\")\n",
        "print(f\"    at L={best_config[0]}, N={best_config[1]}, Î»={best_config[2]:.2e}, batch_size={best_config[3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "eE_rHI_flIU9",
        "outputId": "776a97c4-1e64-4cd2-e5cd-9b0730785397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Final Test RMSE = 2.4445 Â± 0.0354\n"
          ]
        }
      ],
      "source": [
        "# Unpack best hyperparameters\n",
        "L, N, lam, batch_size, _ = best_config\n",
        "\n",
        "# Evaluate over multiple runs for stable estimate (e.g. 5 trials)\n",
        "num_trials = 5\n",
        "rmse_list = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    # Re-initialize model\n",
        "    init_size = len(X_train) // 2\n",
        "    X0, y0 = X_train[:init_size], y_train[:init_size]\n",
        "    \n",
        "    beta_list, P_list, W_list, b_list = init_fit(\n",
        "        X0, y0, X.shape[1], N, L, lam, np.random.default_rng()\n",
        "    )\n",
        "\n",
        "    # Online training with remaining train set\n",
        "    for i in range(init_size, len(X_train), batch_size):\n",
        "        Xk = X_train[i:i+batch_size]\n",
        "        yk = y_train[i:i+batch_size]\n",
        "        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L)\n",
        "\n",
        "    # Evaluate on X_test\n",
        "    pred_test = predict_average(X_test, beta_list, W_list, b_list, L)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, pred_test))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "# Final results\n",
        "mean_rmse = np.mean(rmse_list)\n",
        "std_rmse = np.std(rmse_list)\n",
        "\n",
        "print(f\"\\nðŸ“Š Final Test RMSE = {mean_rmse:.4f} Â± {std_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Daily Demand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature sample:\n",
            "    0   1   2   3   4\n",
            "0  85  92  45  27  31\n",
            "1  85  64  59  32  23\n",
            "2  86  54  33  16  54\n",
            "3  91  78  34  24  36\n",
            "4  87  70  12  28  10\n",
            "5  98  55  13  17  17\n",
            "6  88  62  20  17   9\n",
            "7  88  67  21  11  11\n",
            "8  92  54  22  20   7\n",
            "9  90  60  25  19   5\n",
            "\n",
            "Dataset dimensions: 345 rows Ã— 5 columns\n",
            "\n",
            "Train set: 241 samples\n",
            "Test set : 104 samples\n"
          ]
        }
      ],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Fetch dataset from UCI repo by ID (60 = Daily Demand Forecasting Orders)\n",
        "daily_demand = fetch_ucirepo(id=60)\n",
        "\n",
        "# Step 2: Extract features and target\n",
        "X = daily_demand.data.features.to_numpy()\n",
        "y = daily_demand.data.targets.to_numpy().astype(np.float32).ravel()\n",
        "\n",
        "# Step 3: Metadata & variable info (optional)\n",
        "print(\"\\nFeature sample:\")\n",
        "print(pd.DataFrame(X).head(10))\n",
        "print(f\"\\nDataset dimensions: {X.shape[0]} rows Ã— {X.shape[1]} columns\")\n",
        "\n",
        "# Step 4: 70:30 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42\n",
        ")\n",
        "\n",
        "# Confirm dimensions\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set : {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Best RMSE = 3.0378 Â± 0.3824\n",
            "    at L=7, N=512, Î»=6.40e+01, batch_size=20\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Standardization (fit only on training data)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Hyperparameter search space\n",
        "lam_powers = list(range(-6, 13, 2))                         # exponents for 2^x\n",
        "lam_values = [1 / (2 ** x) for x in lam_powers]             # regularization values\n",
        "L_values = list(range(2, 10))                               # number of hidden layers\n",
        "N_values = [256, 512, 1024]                                 # number of hidden units\n",
        "batch_sizes = [20, 22, 24, 26, 28, 30]                      # online batch sizes\n",
        "\n",
        "# Step 3: Setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "best_rmse = float(\"inf\")\n",
        "best_config = None\n",
        "results = []\n",
        "\n",
        "# Step 4: Grid search\n",
        "for L in L_values:\n",
        "    for N in N_values:\n",
        "        for lam in lam_values:\n",
        "            for batch_size in batch_sizes:\n",
        "                fold_rmse = []\n",
        "                for train_idx, val_idx in kf.split(X_train):\n",
        "                    X_fold_train, X_val = X_train[train_idx], X_train[val_idx]\n",
        "                    y_fold_train, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "                    # Initial batch\n",
        "                    init_size = len(X_fold_train) // 2\n",
        "                    X0, y0 = X_fold_train[:init_size], y_fold_train[:init_size]\n",
        "\n",
        "                    beta_list, P_list, W_list, b_list = init_fit(\n",
        "                        X0, y0, X.shape[1], N, L, lam, rng\n",
        "                    )\n",
        "\n",
        "                    # Online Update\n",
        "                    for i in range(init_size, len(X_fold_train), batch_size):\n",
        "                        Xk = X_fold_train[i:i+batch_size]\n",
        "                        yk = y_fold_train[i:i+batch_size]\n",
        "                        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L)\n",
        "\n",
        "                    # Validation prediction\n",
        "                    pred_val = predict_average(X_val, beta_list, W_list, b_list, L)\n",
        "                    rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n",
        "                    fold_rmse.append(rmse)\n",
        "\n",
        "                mean_rmse = np.mean(fold_rmse)\n",
        "                std_rmse = np.std(fold_rmse)\n",
        "                results.append((L, N, lam, batch_size, mean_rmse, std_rmse))\n",
        "\n",
        "                if mean_rmse < best_rmse:\n",
        "                    best_rmse = mean_rmse\n",
        "                    best_config = (L, N, lam, batch_size, std_rmse)\n",
        "\n",
        "# Step 6: Report\n",
        "print(f\"\\nâœ… Best RMSE = {best_rmse:.4f} Â± {best_config[4]:.4f}\")\n",
        "print(f\"    at L={best_config[0]}, N={best_config[1]}, Î»={best_config[2]:.2e}, batch_size={best_config[3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Final Test RMSE = 2.8548 Â± 0.0226\n"
          ]
        }
      ],
      "source": [
        "# Unpack best hyperparameters\n",
        "L, N, lam, batch_size, _ = best_config\n",
        "\n",
        "# Evaluate over multiple runs for stable estimate (e.g. 5 trials)\n",
        "num_trials = 5\n",
        "rmse_list = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    # Re-initialize model\n",
        "    init_size = len(X_train) // 2\n",
        "    X0, y0 = X_train[:init_size], y_train[:init_size]\n",
        "    \n",
        "    beta_list, P_list, W_list, b_list = init_fit(\n",
        "        X0, y0, X.shape[1], N, L, lam, np.random.default_rng()\n",
        "    )\n",
        "\n",
        "    # Online training with remaining train set\n",
        "    for i in range(init_size, len(X_train), batch_size):\n",
        "        Xk = X_train[i:i+batch_size]\n",
        "        yk = y_train[i:i+batch_size]\n",
        "        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L)\n",
        "\n",
        "    # Evaluate on X_test\n",
        "    pred_test = predict_average(X_test, beta_list, W_list, b_list, L)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, pred_test))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "# Final results\n",
        "mean_rmse = np.mean(rmse_list)\n",
        "std_rmse = np.std(rmse_list)\n",
        "\n",
        "print(f\"\\nðŸ“Š Final Test RMSE = {mean_rmse:.4f} Â± {std_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clasification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Forward pass through all L hidden layers\n",
        "def get_all_H(X, W_list, b_list):\n",
        "    H_list = []\n",
        "    H = sigmoid(X @ W_list[0] + b_list[0])\n",
        "    H_list.append(H)\n",
        "    for l in range(1, len(W_list)):\n",
        "        XH = np.concatenate([X, H_list[-1]], axis=1)\n",
        "        H = sigmoid(XH @ W_list[l] + b_list[l])\n",
        "        H_list.append(H)\n",
        "    return H_list\n",
        "\n",
        "# Construct deep feature matrix D for each layer\n",
        "def get_D_deep(X, H_list, l):\n",
        "    return np.concatenate([X, H_list[l]], axis=1)\n",
        "\n",
        "# One-hot encode y for classification\n",
        "def one_hot_encode(y, num_classes):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "# Initial training for classification\n",
        "def init_fit(X0, y0, in_dim, hid_dim, L, lam, rng, num_classes):\n",
        "    beta_list, P_list, W_list, b_list = [], [], [], []\n",
        "    input_dims = [in_dim] + [in_dim + hid_dim] * (L - 1)\n",
        "\n",
        "    for l in range(L):\n",
        "        W = rng.standard_normal((input_dims[l], hid_dim))\n",
        "        b = rng.standard_normal((hid_dim,))\n",
        "        W_list.append(W)\n",
        "        b_list.append(b)\n",
        "\n",
        "    H_list = get_all_H(X0, W_list, b_list)\n",
        "    Y0 = one_hot_encode(y0, num_classes)\n",
        "\n",
        "    for l in range(L):\n",
        "        D = get_D_deep(X0, H_list, l)\n",
        "        P = np.linalg.inv(D.T @ D + lam * np.eye(D.shape[1]))\n",
        "        beta = P @ D.T @ Y0\n",
        "        P_list.append(P)\n",
        "        beta_list.append(beta)\n",
        "\n",
        "    return beta_list, P_list, W_list, b_list\n",
        "\n",
        "# Online update with new batch for classification\n",
        "def partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L, num_classes):\n",
        "    H_list = get_all_H(Xk, W_list, b_list)\n",
        "    Yk = one_hot_encode(yk, num_classes)\n",
        "\n",
        "    for l in range(L):\n",
        "        Dk = get_D_deep(Xk, H_list, l)\n",
        "        P = P_list[l]\n",
        "        beta = beta_list[l]\n",
        "        P_new = P - P @ Dk.T @ np.linalg.inv(np.eye(Dk.shape[0]) + Dk @ P @ Dk.T) @ Dk @ P\n",
        "        beta_new = beta + P_new @ Dk.T @ (Yk - Dk @ beta)\n",
        "        P_list[l] = P_new\n",
        "        beta_list[l] = beta_new\n",
        "\n",
        "# Predict using majority vote from L layers\n",
        "def predict_majority(X, beta_list, W_list, b_list, L):\n",
        "    H_list = get_all_H(X, W_list, b_list)\n",
        "    preds = []\n",
        "\n",
        "    for l in range(L):\n",
        "        D = get_D_deep(X, H_list, l)\n",
        "        out = D @ beta_list[l]\n",
        "        preds.append(np.argmax(out, axis=1))  # class prediction per layer\n",
        "\n",
        "    preds = np.array(preds)  # shape: (L, n_samples)\n",
        "    final_preds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        votes = preds[:, i]\n",
        "        final_preds.append(np.bincount(votes).argmax())  # majority voting\n",
        "\n",
        "    return np.array(final_preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature sample:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n",
            "5                5.4               3.9                1.7               0.4\n",
            "6                4.6               3.4                1.4               0.3\n",
            "7                5.0               3.4                1.5               0.2\n",
            "8                4.4               2.9                1.4               0.2\n",
            "9                4.9               3.1                1.5               0.1\n",
            "\n",
            "Dataset dimensions: 150 rows Ã— 4 columns\n",
            "\n",
            "Train set: 105 samples\n",
            "Test set : 45 samples\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load original Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data   # shape (150, 4)\n",
        "y = data.target # labels: 0, 1, 2\n",
        "\n",
        "# Print basic info\n",
        "print(\"\\nFeature sample:\")\n",
        "print(pd.DataFrame(X, columns=data.feature_names).head(10))\n",
        "print(f\"\\nDataset dimensions: {X.shape[0]} rows Ã— {X.shape[1]} columns\")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Confirm\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set : {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Best Accuracy = 0.9905 Â± 0.0190\n",
            "    at L=6, N=512, Î»=2.44e-04, batch_size=28\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Step 1: Standardize input features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Hyperparameter space\n",
        "lam_powers = list(range(-6, 13, 2))\n",
        "lam_values = [1 / (2 ** x) for x in lam_powers]\n",
        "L_values = list(range(2, 10))\n",
        "N_values = [256, 512, 1024]\n",
        "batch_sizes = [20, 22, 24, 26, 28, 30]\n",
        "\n",
        "# Step 3: Setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rng = np.random.default_rng(42)\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "best_acc = -1\n",
        "best_config = None\n",
        "results = []\n",
        "\n",
        "# Step 4: Grid search\n",
        "for L in L_values:\n",
        "    for N in N_values:\n",
        "        for lam in lam_values:\n",
        "            for batch_size in batch_sizes:\n",
        "                fold_acc = []\n",
        "\n",
        "                for train_idx, val_idx in kf.split(X_train):\n",
        "                    X_fold_train, X_val = X_train[train_idx], X_train[val_idx]\n",
        "                    y_fold_train, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "                    init_size = len(X_fold_train) // 2\n",
        "                    X0, y0 = X_fold_train[:init_size], y_fold_train[:init_size]\n",
        "\n",
        "                    beta_list, P_list, W_list, b_list = init_fit(\n",
        "                        X0, y0, X.shape[1], N, L, lam, rng, num_classes\n",
        "                    )\n",
        "\n",
        "                    for i in range(init_size, len(X_fold_train), batch_size):\n",
        "                        Xk = X_fold_train[i:i+batch_size]\n",
        "                        yk = y_fold_train[i:i+batch_size]\n",
        "                        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L, num_classes)\n",
        "\n",
        "                    pred_val = predict_majority(X_val, beta_list, W_list, b_list, L)\n",
        "                    acc = np.mean(pred_val == y_val)\n",
        "                    fold_acc.append(acc)\n",
        "\n",
        "                mean_acc = np.mean(fold_acc)\n",
        "                std_acc = np.std(fold_acc)\n",
        "                results.append((L, N, lam, batch_size, mean_acc, std_acc))\n",
        "\n",
        "                if mean_acc > best_acc:\n",
        "                    best_acc = mean_acc\n",
        "                    best_config = (L, N, lam, batch_size, std_acc)\n",
        "\n",
        "# Final result\n",
        "print(f\"\\nâœ… Best Accuracy = {best_acc:.4f} Â± {best_config[4]:.4f}\")\n",
        "print(f\"    at L={best_config[0]}, N={best_config[1]}, Î»={best_config[2]:.2e}, batch_size={best_config[3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Final Test Accuracy = 0.9156 Â± 0.0089\n"
          ]
        }
      ],
      "source": [
        "# Unpack best hyperparameters\n",
        "L, N, lam, batch_size, _ = best_config\n",
        "\n",
        "# Number of evaluation runs (for statistical confidence)\n",
        "num_trials = 5\n",
        "acc_list = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    # Initial batch: first 50% of X_train\n",
        "    init_size = len(X_train) // 2\n",
        "    X0, y0 = X_train[:init_size], y_train[:init_size]\n",
        "\n",
        "    # Train initial model\n",
        "    beta_list, P_list, W_list, b_list = init_fit(\n",
        "        X0, y0, X.shape[1], N, L, lam, np.random.default_rng(), num_classes\n",
        "    )\n",
        "\n",
        "    # Online learning with remaining batches\n",
        "    for i in range(init_size, len(X_train), batch_size):\n",
        "        Xk = X_train[i:i+batch_size]\n",
        "        yk = y_train[i:i+batch_size]\n",
        "        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L, num_classes)\n",
        "\n",
        "    # Predict and evaluate on test set\n",
        "    pred_test = predict_majority(X_test, beta_list, W_list, b_list, L)\n",
        "    acc = np.mean(pred_test == y_test)\n",
        "    acc_list.append(acc)\n",
        "\n",
        "# Final averaged accuracy and standard deviation\n",
        "mean_acc = np.mean(acc_list)\n",
        "std_acc = np.std(acc_list)\n",
        "\n",
        "print(f\"\\nðŸ“Š Final Test Accuracy = {mean_acc:.4f} Â± {std_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Breast Cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature sample:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "5        12.45         15.70           82.57      477.1          0.12780   \n",
            "6        18.25         19.98          119.60     1040.0          0.09463   \n",
            "7        13.71         20.83           90.20      577.9          0.11890   \n",
            "8        13.00         21.82           87.50      519.8          0.12730   \n",
            "9        12.46         24.04           83.97      475.9          0.11860   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760         0.30010              0.14710         0.2419   \n",
            "1           0.07864         0.08690              0.07017         0.1812   \n",
            "2           0.15990         0.19740              0.12790         0.2069   \n",
            "3           0.28390         0.24140              0.10520         0.2597   \n",
            "4           0.13280         0.19800              0.10430         0.1809   \n",
            "5           0.17000         0.15780              0.08089         0.2087   \n",
            "6           0.10900         0.11270              0.07400         0.1794   \n",
            "7           0.16450         0.09366              0.05985         0.2196   \n",
            "8           0.19320         0.18590              0.09353         0.2350   \n",
            "9           0.23960         0.22730              0.08543         0.2030   \n",
            "\n",
            "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
            "0                 0.07871  ...         25.38          17.33           184.60   \n",
            "1                 0.05667  ...         24.99          23.41           158.80   \n",
            "2                 0.05999  ...         23.57          25.53           152.50   \n",
            "3                 0.09744  ...         14.91          26.50            98.87   \n",
            "4                 0.05883  ...         22.54          16.67           152.20   \n",
            "5                 0.07613  ...         15.47          23.75           103.40   \n",
            "6                 0.05742  ...         22.88          27.66           153.20   \n",
            "7                 0.07451  ...         17.06          28.14           110.60   \n",
            "8                 0.07389  ...         15.49          30.73           106.20   \n",
            "9                 0.08243  ...         15.09          40.68            97.65   \n",
            "\n",
            "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
            "0      2019.0            0.1622             0.6656           0.7119   \n",
            "1      1956.0            0.1238             0.1866           0.2416   \n",
            "2      1709.0            0.1444             0.4245           0.4504   \n",
            "3       567.7            0.2098             0.8663           0.6869   \n",
            "4      1575.0            0.1374             0.2050           0.4000   \n",
            "5       741.6            0.1791             0.5249           0.5355   \n",
            "6      1606.0            0.1442             0.2576           0.3784   \n",
            "7       897.0            0.1654             0.3682           0.2678   \n",
            "8       739.3            0.1703             0.5401           0.5390   \n",
            "9       711.4            0.1853             1.0580           1.1050   \n",
            "\n",
            "   worst concave points  worst symmetry  worst fractal dimension  \n",
            "0                0.2654          0.4601                  0.11890  \n",
            "1                0.1860          0.2750                  0.08902  \n",
            "2                0.2430          0.3613                  0.08758  \n",
            "3                0.2575          0.6638                  0.17300  \n",
            "4                0.1625          0.2364                  0.07678  \n",
            "5                0.1741          0.3985                  0.12440  \n",
            "6                0.1932          0.3063                  0.08368  \n",
            "7                0.1556          0.3196                  0.11510  \n",
            "8                0.2060          0.4378                  0.10720  \n",
            "9                0.2210          0.4366                  0.20750  \n",
            "\n",
            "[10 rows x 30 columns]\n",
            "\n",
            "Dataset dimensions: 569 rows Ã— 30 columns\n",
            "\n",
            "Train set: 398 samples\n",
            "Test set : 171 samples\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # 0 = malignant, 1 = benign\n",
        "\n",
        "# Print basic info\n",
        "print(\"\\nFeature sample:\")\n",
        "print(pd.DataFrame(X, columns=data.feature_names).head(10))\n",
        "print(f\"\\nDataset dimensions: {X.shape[0]} rows Ã— {X.shape[1]} columns\")\n",
        "\n",
        "# 70:30 Train-test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Confirm splits\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set : {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Best Accuracy = 0.9799 Â± 0.0170\n",
            "    at L=4, N=1024, Î»=4.00e+00, batch_size=26\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Step 1: Standardize input features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 2: Hyperparameter space\n",
        "lam_powers = list(range(-6, 13, 2))\n",
        "lam_values = [1 / (2 ** x) for x in lam_powers]\n",
        "L_values = list(range(2, 10))\n",
        "N_values = [256, 512, 1024]\n",
        "batch_sizes = [20, 22, 24, 26, 28, 30]\n",
        "\n",
        "# Step 3: Setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rng = np.random.default_rng(42)\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "best_acc = -1\n",
        "best_config = None\n",
        "results = []\n",
        "\n",
        "# Step 4: Grid search\n",
        "for L in L_values:\n",
        "    for N in N_values:\n",
        "        for lam in lam_values:\n",
        "            for batch_size in batch_sizes:\n",
        "                fold_acc = []\n",
        "\n",
        "                for train_idx, val_idx in kf.split(X_train):\n",
        "                    X_fold_train, X_val = X_train[train_idx], X_train[val_idx]\n",
        "                    y_fold_train, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "                    init_size = len(X_fold_train) // 2\n",
        "                    X0, y0 = X_fold_train[:init_size], y_fold_train[:init_size]\n",
        "\n",
        "                    beta_list, P_list, W_list, b_list = init_fit(\n",
        "                        X0, y0, X.shape[1], N, L, lam, rng, num_classes\n",
        "                    )\n",
        "\n",
        "                    for i in range(init_size, len(X_fold_train), batch_size):\n",
        "                        Xk = X_fold_train[i:i+batch_size]\n",
        "                        yk = y_fold_train[i:i+batch_size]\n",
        "                        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L, num_classes)\n",
        "\n",
        "                    pred_val = predict_majority(X_val, beta_list, W_list, b_list, L)\n",
        "                    acc = np.mean(pred_val == y_val)\n",
        "                    fold_acc.append(acc)\n",
        "\n",
        "                mean_acc = np.mean(fold_acc)\n",
        "                std_acc = np.std(fold_acc)\n",
        "                results.append((L, N, lam, batch_size, mean_acc, std_acc))\n",
        "\n",
        "                if mean_acc > best_acc:\n",
        "                    best_acc = mean_acc\n",
        "                    best_config = (L, N, lam, batch_size, std_acc)\n",
        "\n",
        "# Final result\n",
        "print(f\"\\nâœ… Best Accuracy = {best_acc:.4f} Â± {best_config[4]:.4f}\")\n",
        "print(f\"    at L={best_config[0]}, N={best_config[1]}, Î»={best_config[2]:.2e}, batch_size={best_config[3]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Final Test Accuracy = 0.9731 Â± 0.0047\n"
          ]
        }
      ],
      "source": [
        "# Unpack best hyperparameters\n",
        "L, N, lam, batch_size, _ = best_config\n",
        "\n",
        "# Number of evaluation runs (for statistical confidence)\n",
        "num_trials = 5\n",
        "acc_list = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    # Initial batch: first 50% of X_train\n",
        "    init_size = len(X_train) // 2\n",
        "    X0, y0 = X_train[:init_size], y_train[:init_size]\n",
        "\n",
        "    # Train initial model\n",
        "    beta_list, P_list, W_list, b_list = init_fit(\n",
        "        X0, y0, X.shape[1], N, L, lam, np.random.default_rng(), num_classes\n",
        "    )\n",
        "\n",
        "    # Online learning with remaining batches\n",
        "    for i in range(init_size, len(X_train), batch_size):\n",
        "        Xk = X_train[i:i+batch_size]\n",
        "        yk = y_train[i:i+batch_size]\n",
        "        partial_fit(Xk, yk, beta_list, P_list, W_list, b_list, L, num_classes)\n",
        "\n",
        "    # Predict and evaluate on test set\n",
        "    pred_test = predict_majority(X_test, beta_list, W_list, b_list, L)\n",
        "    acc = np.mean(pred_test == y_test)\n",
        "    acc_list.append(acc)\n",
        "\n",
        "# Final averaged accuracy and standard deviation\n",
        "mean_acc = np.mean(acc_list)\n",
        "std_acc = np.std(acc_list)\n",
        "\n",
        "print(f\"\\nðŸ“Š Final Test Accuracy = {mean_acc:.4f} Â± {std_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "babyEmotion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
